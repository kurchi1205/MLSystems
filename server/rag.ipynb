{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage:\n",
    "1. Run `python api.py` to start the server.\n",
    "2. Run this script to test the server.\n",
    "3. You don't need to consider quota limit for rag.\n",
    "\n",
    "You MUST keep the logs of this file in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, base_url: str = \"http://localhost:8000\"):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        \n",
    "    def get_embedding(self, prompt: str) -> Dict:\n",
    "        \"\"\"get embedding\"\"\"\n",
    "        # TODO: get embedding function call\n",
    "        # you need to post a request to the server\n",
    "        # and parse the response in sync way\n",
    "        # the response is a json with the following format:\n",
    "        # {\n",
    "        #     \"embedding\": List[float],\n",
    "        # }\n",
    "        # ==== start your code here ====\n",
    "        data = {\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "        response = requests.post(self.base_url + \"/get_embedding\", json=data)\n",
    "        embedding = response.text\n",
    "        result = {\n",
    "            \"embedding\": eval(embedding)[\"embedding\"]\n",
    "        }\n",
    "        return result\n",
    "        # ==== end of your code ====\n",
    "\n",
    "    def generate(self, prompt: str) -> Dict:\n",
    "        \"\"\"generate\"\"\"\n",
    "        # TODO: generate function call\n",
    "        # you need to post a request to the server\n",
    "        # and parse the response in async way\n",
    "        # the response is a json with the following format:\n",
    "        # {\n",
    "        #     \"status\": \"success\" or \"error\",\n",
    "        #     \"text\": \"the generated text\"\n",
    "        # }\n",
    "        # ==== start your code here ====\n",
    "        data = {\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "        response = requests.post(self.base_url + \"/generate\", json=data)\n",
    "        result = response.text\n",
    "        result = {\n",
    "            \"status\": eval(result)[\"status\"],\n",
    "            \"text\": eval(result)[\"text\"]\n",
    "        }\n",
    "        return result\n",
    "        # ==== end of your code ====\n",
    "\n",
    "\n",
    "server = Server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE = {}\n",
    "\n",
    "\n",
    "def construct_database(file_path: str):\n",
    "    # TODO: construct database\n",
    "    # you need to read the file and split the file into several paragraphs\n",
    "    # then construct the database by calling the get_embedding function: emb = server.get_embedding(text)[\"embedding\"]\n",
    "    # the database is a dictionary with the following format:\n",
    "    # {\n",
    "    #     \"prompt1\": torch.Tensor of shape [dim,],\n",
    "    #     \"prompt2\": torch.Tensor of shape [dim,],\n",
    "    #     ...\n",
    "    # }\n",
    "    # ==== start your code here ====\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n",
    "    for paragraph in paragraphs:\n",
    "        emb = server.get_embedding(paragraph)[\"embedding\"]\n",
    "        DATABASE[paragraph] = emb\n",
    "\n",
    "    # ==== end of your code ====\n",
    "\n",
    "construct_database(\"openai_wiki.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When was Sam Altman removed as CEO?\n",
      "\n",
      "RAG Prompt: Controversies\n",
      "Firing of Altman\n",
      "Further information: Removal of Sam Altman from OpenAI\n",
      "On November 17, 2023, Sam Altman was removed as CEO when its board of directors (composed of Helen Toner, Ilya Sutskever, Adam D'Angelo and Tasha McCauley) cited a lack of confidence in him. Chief Technology Officer Mira Murati took over as interim CEO. Greg Brockman, the president of OpenAI, was also removed as chairman of the board[242][243] and resigned from the company's presidency shortly thereafter.[244] Three senior OpenAI researchers subsequently resigned: director of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Madry, and researcher Szymon Sidor.[245][246] On June 13, 2024, OpenAI announced that Paul Nakasone, the former head of the NSA was joining the company's board of directors. Nakasone also joined the company's security subcommittee.[95] In October 2023, Sam Altman and Peng Xiao, CEO of the Emirati AI firm G42, announced Open AI would let G42 deploy Open AI technology.[67] On September 25, OpenAI's Chief Technology Officer (CTO) Mira Murati announced her departure from the company to \"create the time and space to do my own exploration\".[101] It had previously been reported Murati was among those who expressed concerns to the Board about Altman.[102] On November 18, 2023, there were reportedly talks of Altman returning as CEO amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who objected to Altman's departure.[247] Although Altman himself spoke in favor of returning to OpenAI, he has since stated that he considered starting a new company and bringing former OpenAI employees with him if talks to reinstate him didn't work out.[248] The board members agreed \"in principle\" to resign if Altman returned.[249] On November 19, 2023, negotiations with Altman to return failed and Murati was replaced by Emmett Shear as interim CEO.[250] The board initially contacted Anthropic CEO Dario Amodei (a former OpenAI executive) about replacing Altman, and proposed a merger of the two companies, but both offers were declined.[251]When was Sam Altman removed as CEO?\n",
      "\n",
      "Answer:  November 17, 2023.\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    # TODO: cosine similarity function\n",
    "    # you need to calculate the cosine similarity between two tensors\n",
    "    # ==== start your code here ====\n",
    "    dot_product = a@b.T\n",
    "    magnitude_a = torch.norm(a)\n",
    "    magnitude_b = torch.norm(b)\n",
    "    cos_sim = dot_product / (magnitude_a * magnitude_b)\n",
    "    return cos_sim.item()\n",
    "    # ==== end of your code ====\n",
    "\n",
    "\n",
    "def rag(prompt: str):\n",
    "    prompt_embedding = torch.tensor(server.get_embedding(prompt)[\"embedding\"]).cuda()\n",
    "\n",
    "    topk = 5\n",
    "    # TODO: rag prompt\n",
    "    # you first need to find the topk similar prompt in the database by calculating the cosine similarity\n",
    "    # then you need to construct the rag prompt by adding the topk similar prompt with the original prompt\n",
    "    # ==== start your code here ====\n",
    "    \n",
    "    rag_prompt = \"\"\n",
    "    prompts = []\n",
    "    similarities = []\n",
    "    for pr, emb in DATABASE.items():\n",
    "        prompts.append(pr)\n",
    "        similarities.append(cosine_similarity(prompt_embedding.detach().cpu(), torch.tensor(emb)))\n",
    "    \n",
    "\n",
    "    topk_indices = torch.topk(torch.tensor(similarities), topk).indices\n",
    "    selected_prompts = \" \".join([prompts[i] for i in topk_indices])\n",
    "    rag_prompt = selected_prompts + prompt\n",
    "    \n",
    "    # ==== end of your code ====\n",
    "    answer = server.generate(rag_prompt)['text']\n",
    "    print(f\"Question: {prompt}\\n\\nRAG Prompt: {rag_prompt}\\n\\nAnswer: {answer}\")\n",
    "\n",
    "rag(\"When was Sam Altman removed as CEO?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
